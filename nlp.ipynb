{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "id": "uNk0DROE3IqR",
    "outputId": "72953371-39a1-49c2-d53e-181e80090643"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "Y7pZd42J3IqU",
    "outputId": "f90b40e8-4eb0-41e4-f226-d1248b87522c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18759</th>\n",
       "      <td>18759</td>\n",
       "      <td>we now have definitive proof the @user does no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13410</th>\n",
       "      <td>13410</td>\n",
       "      <td>@user we've had some of that round here - but ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>3597</td>\n",
       "      <td>@user @user @user @user @user @user @user @use...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32156</th>\n",
       "      <td>32156</td>\n",
       "      <td>#angels #archangels from on high here to help ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10556</th>\n",
       "      <td>10556</td>\n",
       "      <td>#hapoyfathersday : #kimkardashian wishes #kany...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0                                               text  result\n",
       "18759       18759  we now have definitive proof the @user does no...       1\n",
       "13410       13410  @user we've had some of that round here - but ...       1\n",
       "3597         3597  @user @user @user @user @user @user @user @use...       1\n",
       "32156       32156  #angels #archangels from on high here to help ...       1\n",
       "10556       10556  #hapoyfathersday : #kimkardashian wishes #kany...       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"text_labelled.csv\")\n",
    "train, test = sklearn.model_selection.train_test_split(data, random_state = 0)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "yQFxgT_H3IqX",
    "outputId": "36be681f-130b-4ef7-b857-e55a222f4132"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    23307\n",
       "0     2725\n",
       "Name: result, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = train.drop(['Unnamed: 0'], axis=1)\n",
    "test = test.drop(['Unnamed: 0'], axis=1)\n",
    "class_count = train[\"result\"].value_counts()\n",
    "class_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSnmLi4L3Iqa"
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "KIxrF9b73Iqd",
    "outputId": "7536c5c9-ce3d-4636-d8d9-f1bbdb9d5bed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18759</th>\n",
       "      <td>we now have definitive proof the user does not...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13410</th>\n",
       "      <td>user weve had some of that round here  but you...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>user user user user user user user user   sund...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32156</th>\n",
       "      <td>angels archangels from on high here to help yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10556</th>\n",
       "      <td>hapoyfathersday  kimkardashian wishes kanyewes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  result\n",
       "18759  we now have definitive proof the user does not...       1\n",
       "13410  user weve had some of that round here  but you...       1\n",
       "3597   user user user user user user user user   sund...       1\n",
       "32156  angels archangels from on high here to help yo...       1\n",
       "10556  hapoyfathersday  kimkardashian wishes kanyewes...       1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"text\"] = train[\"text\"].apply(preprocess)\n",
    "test[\"text\"] = test[\"text\"].apply(preprocess)\n",
    "cv_acc = {}\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fDMr1yehY8cQ"
   },
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q2VM3_aP3Iqk"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "import fastText\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def train_fasttext(train):\n",
    "    with open('new_text.csv', mode='w', encoding=\"utf8\", newline='') as new_text:\n",
    "        for i in range(train.shape[0]):\n",
    "            employee_writer = csv.writer(new_text, delimiter=',', quotechar='\"')\n",
    "            employee_writer.writerow(train.iloc[i])\n",
    "        \n",
    "    def transform_instance(row):\n",
    "        cur_row = []\n",
    "        label = \"__label__\" + row[1]  \n",
    "        cur_row.append(label)\n",
    "        cur_row.extend(nltk.word_tokenize(row[0]))\n",
    "        return cur_row\n",
    "\n",
    "    def preprocess(input_file, output_file, keep=1):\n",
    "        with open(output_file, 'w', encoding=\"utf8\") as csvoutfile:\n",
    "            csv_writer = csv.writer(csvoutfile, delimiter=' ', lineterminator='\\n')\n",
    "            with open(input_file, 'r', newline='', encoding=\"utf8\") as csvinfile:\n",
    "                csv_reader = csv.reader(csvinfile, delimiter=',', quotechar='\"')\n",
    "                for row in csv_reader:\n",
    "                    if row[1] in [\"0\",\"1\"]:\n",
    "                        row_output = transform_instance(row)\n",
    "                        csv_writer.writerow(row_output)\n",
    "    \n",
    "    preprocess('new_text.csv', 'trial.train')\n",
    "    hyper_params = {\"lr\": 0.01, \"epoch\": 20, \"wordNgrams\": 2, \"dim\": 20}     \n",
    "        \n",
    "    model = fastText.train_supervised(input=\"./trial.train\", **hyper_params)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aVU50L8VY8cT"
   },
   "outputs": [],
   "source": [
    "def validate_fasttext(val, model):\n",
    "    probs = val[\"text\"].apply(model.predict)\n",
    "    pred = []\n",
    "    for i in probs:\n",
    "        pred.append(int(i[0][0][-1]))\n",
    "    \n",
    "    score = f1_score(val[\"result\"], pred)\n",
    "    cv_acc[\"FastText\"] = score\n",
    "    print(\"CV Accuracy: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9-o9C4tY8cb"
   },
   "outputs": [],
   "source": [
    "def test_fasttext(test, model):\n",
    "    probs = test[\"text\"].apply(model.predict)\n",
    "    pred = []\n",
    "    for i in probs:\n",
    "        pred.append(int(i[0][0][-1]))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "id": "vuJhi7xb3Iqn",
    "outputId": "794d88db-d4cb-4c9a-e9a0-ab34d417cb21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.9592406476828587\n"
     ]
    }
   ],
   "source": [
    "model1 = train_fasttext(train)\n",
    "validate_fasttext(test, model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RcaAAkvZ3Iqp"
   },
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VcNbo3Cb3Iqs"
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def train_logistic(train):\n",
    "    vectorizer = TfidfVectorizer(use_idf=True, max_df=0.95)\n",
    "    vectorizer.fit_transform(train[\"text\"].values)\n",
    "    train_vectorized=vectorizer.transform(train[\"text\"].values)\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    model = OneVsRestClassifier(logreg)\n",
    "    model.fit(train_vectorized, train['result'])\n",
    "    return vectorizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-9rS0tv_Y8cy"
   },
   "outputs": [],
   "source": [
    "def validate_logistic(val, model, vectorizer):\n",
    "    val_vectorized = vectorizer.transform(val[\"text\"].values)\n",
    "\n",
    "    pred = model.predict(val_vectorized)\n",
    "    score = f1_score(val[\"result\"], pred)\n",
    "    cv_acc[\"Logistic Regression\"] = score\n",
    "    print(\"CV Accuracy: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "noOyNhx4Y8c6"
   },
   "outputs": [],
   "source": [
    "def test_logistic(test, model, vectorizer):\n",
    "    test_vectorized = vectorizer.transform(test[\"text\"].values)\n",
    "    \n",
    "    pred = model.predict(test_vectorized)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MDq8ZBilY8c9",
    "outputId": "da1fdbf1-6203-4681-fd5b-dec77533844f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.9575256204469688\n"
     ]
    }
   ],
   "source": [
    "vectorizer, model2 = train_logistic(train)\n",
    "validate_logistic(test, model2, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5qPXWxlh3Iqx"
   },
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mYH9pylr3Iqz"
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def train_svc(train):\n",
    "    vectorizer = TfidfVectorizer(use_idf=True, max_df=0.95)\n",
    "    vectorizer.fit_transform(train[\"text\"].values)\n",
    "    train_vectorized=vectorizer.transform(train[\"text\"].values)\n",
    "    \n",
    "    svc = LinearSVC(dual=False)\n",
    "    svc.fit(train_vectorized, train['result'])\n",
    "    return vectorizer, svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CvbGTSD4Y8dX"
   },
   "outputs": [],
   "source": [
    "def validate_svc(val, model, vectorizer):\n",
    "    val_vectorized = vectorizer.transform(val[\"text\"].values)\n",
    "    \n",
    "    pred = model.predict(val_vectorized)\n",
    "    score = f1_score(val[\"result\"], pred)\n",
    "    cv_acc[\"Linear SVM\"] = score\n",
    "    print(\"CV Accuracy: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "yx2-YDSy3Iq2",
    "outputId": "afcba65c-e8ee-4015-8a3b-66701a51a3a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.9671036047390976\n"
     ]
    }
   ],
   "source": [
    "vectorizer, model3 = train_svc(train)\n",
    "validate_svc(test, model3, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PMeWul6R3Iq7"
   },
   "source": [
    "## Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CuRKzX583Iq9"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, BatchNormalization\n",
    "from tensorflow.keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\n",
    "from tensorflow.keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.keras.engine import InputSpec, Layer\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAvykOx93Iq_"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def train_BiLSTM(train):\n",
    "    tk = Tokenizer(lower = True, filters='')\n",
    "    tk.fit_on_texts(train[\"text\"])\n",
    "    train_tokenized = tk.texts_to_sequences(train['text'])\n",
    "\n",
    "    max_len = 100\n",
    "    X_train = pad_sequences(train_tokenized, maxlen = max_len)\n",
    "\n",
    "    embed_size = 300\n",
    "    max_features = 100000\n",
    "    embedding_path = \"./wiki-news-300d-1M-subword.vec\"\n",
    "\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "    embedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path, encoding=\"utf8\"))\n",
    "\n",
    "    word_index = tk.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.zeros((nb_words + 1, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    ohe = OneHotEncoder(sparse=False)\n",
    "    y_ohe = ohe.fit_transform(train['result'].values.reshape(-1, 1))\n",
    "    \n",
    "    def build_model(lr=0.0, lr_d=0.0, units=0, spatial_dr=0.0, kernel_size1=3, kernel_size2=2, dense_units=128, dr=0.1, conv_size=32):\n",
    "        file_path = \"best_model.hdf5\"\n",
    "        check_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n",
    "                                      save_best_only = True, mode = \"min\")\n",
    "        early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n",
    "\n",
    "        inp = Input(shape = (max_len,))\n",
    "        x = Embedding(39457, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n",
    "        x1 = SpatialDropout1D(spatial_dr)(x)\n",
    "\n",
    "        x_gru = Bidirectional(LSTM(units, return_sequences = True))(x1)\n",
    "        x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_gru)\n",
    "        avg_pool1_gru = GlobalAveragePooling1D()(x1)\n",
    "        max_pool1_gru = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "        x_lstm = Bidirectional(LSTM(units, return_sequences = True))(x1)\n",
    "        x1 = Conv1D(conv_size, kernel_size=kernel_size1, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n",
    "        avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n",
    "        max_pool1_lstm = GlobalMaxPooling1D()(x1)\n",
    "\n",
    "        x = concatenate([avg_pool1_gru, max_pool1_gru,\n",
    "                        avg_pool1_lstm, max_pool1_lstm])\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dropout(dr)(Dense(dense_units, activation='relu') (x))\n",
    "        x = Dense(2, activation = \"sigmoid\")(x)\n",
    "\n",
    "        model = Model(inputs = inp, outputs = x)\n",
    "        model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n",
    "\n",
    "        history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 5, validation_split=0.1, \n",
    "                            verbose = 1, callbacks = [check_point, early_stop])\n",
    "        model = load_model(file_path)\n",
    "        return model\n",
    "    \n",
    "    model = build_model(lr = 1e-3, lr_d = 1e-10, units = 64, spatial_dr = 0.3, \n",
    "                      kernel_size1=3, kernel_size2=2, dense_units=32, dr=0.1, conv_size=32)\n",
    "    return tk, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BabAgrMRY8dq"
   },
   "outputs": [],
   "source": [
    "def validate_BiLSTM(val, model, tk):\n",
    "    max_len = 100\n",
    "    test_tokenized = tk.texts_to_sequences(val[\"text\"])\n",
    "    input_test = pad_sequences(test_tokenized, maxlen = max_len)\n",
    "\n",
    "    probs = model.predict(input_test)\n",
    "    pred = []\n",
    "    for i in probs:\n",
    "        if i[0] > i[1]:\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "        \n",
    "    score = f1_score(val[\"result\"], pred)\n",
    "    cv_acc[\"Bi-LSTM\"] = score\n",
    "    print(\"CV Accuracy: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kh7adFeVY8dt"
   },
   "outputs": [],
   "source": [
    "def test_BiLSTM(test, model, tk):\n",
    "    max_len = 100\n",
    "    test_tokenized = tk.texts_to_sequences(test[\"text\"])\n",
    "    input_test = pad_sequences(test_tokenized, maxlen = max_len)\n",
    "\n",
    "    probs = model.predict(input_test)\n",
    "    pred = []\n",
    "    for i in probs:\n",
    "        if i[0] > i[1]:\n",
    "            pred.append(0)\n",
    "        else:\n",
    "            pred.append(1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gX_W5GPYY8dv",
    "outputId": "f3217194-aa4b-45ea-ee24-09f3042a52d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 23428 samples, validate on 2604 samples\n",
      "Epoch 1/5\n",
      "23424/23428 [============================>.] - ETA: 0s - loss: 0.3086 - accuracy: 0.8816\n",
      "Epoch 00001: val_loss improved from inf to 0.30378, saving model to best_model.hdf5\n",
      "23428/23428 [==============================] - 135s 6ms/sample - loss: 0.3086 - accuracy: 0.8817 - val_loss: 0.3038 - val_accuracy: 0.8938\n",
      "Epoch 2/5\n",
      "23424/23428 [============================>.] - ETA: 0s - loss: 0.2517 - accuracy: 0.9002\n",
      "Epoch 00002: val_loss improved from 0.30378 to 0.25823, saving model to best_model.hdf5\n",
      "23428/23428 [==============================] - 126s 5ms/sample - loss: 0.2517 - accuracy: 0.9002 - val_loss: 0.2582 - val_accuracy: 0.8948\n",
      "Epoch 3/5\n",
      "23424/23428 [============================>.] - ETA: 0s - loss: 0.2371 - accuracy: 0.9063\n",
      "Epoch 00003: val_loss did not improve from 0.25823\n",
      "23428/23428 [==============================] - 128s 5ms/sample - loss: 0.2371 - accuracy: 0.9063 - val_loss: 0.3040 - val_accuracy: 0.9115\n",
      "Epoch 4/5\n",
      "23424/23428 [============================>.] - ETA: 0s - loss: 0.2305 - accuracy: 0.9092\n",
      "Epoch 00004: val_loss did not improve from 0.25823\n",
      "23428/23428 [==============================] - 132s 6ms/sample - loss: 0.2305 - accuracy: 0.9092 - val_loss: 0.3488 - val_accuracy: 0.9090\n",
      "Epoch 5/5\n",
      "23424/23428 [============================>.] - ETA: 0s - loss: 0.2175 - accuracy: 0.9123\n",
      "Epoch 00005: val_loss did not improve from 0.25823\n",
      "23428/23428 [==============================] - 132s 6ms/sample - loss: 0.2175 - accuracy: 0.9123 - val_loss: 0.3057 - val_accuracy: 0.9092\n",
      "CV Accuracy: 0.9479768786127168\n"
     ]
    }
   ],
   "source": [
    "tokenizer, model4 = train_BiLSTM(train)\n",
    "validate_BiLSTM(test, model4, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJcZ6Qnt3IrI"
   },
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-2IFPmK_AuzF"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch-pretrained-bert pytorch-nlp\n",
    "import io\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm, trange\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "89GgkBVyAu2b"
   },
   "outputs": [],
   "source": [
    "def train_BERT(train):\n",
    "    y_train = train.result.values\n",
    "    X_train = train.text.values\n",
    "\n",
    "    X_train = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in X_train]\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    tokenized_train = [tokenizer.tokenize(sent) for sent in X_train]\n",
    "    \n",
    "    MAX_LEN = 128\n",
    "    train_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_train]\n",
    "    train_ids = pad_sequences(train_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    train_masks = []\n",
    "    for seq in train_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        train_masks.append(seq_mask)\n",
    "        \n",
    "    train_inputs = torch.tensor(train_ids).to(torch.int64)\n",
    "    train_labels = torch.tensor(y_train).to(torch.int64)\n",
    "    train_masks = torch.tensor(train_masks).to(torch.int64)\n",
    "    \n",
    "    batch_size = 32\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    \n",
    "    model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "    model.cuda()\n",
    "    \n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "    \n",
    "    optimizer = BertAdam(optimizer_grouped_parameters, lr=2e-5, warmup=.1)\n",
    "    \n",
    "    t = [] \n",
    "    train_loss_set = []\n",
    "    epochs = 4\n",
    "\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):\n",
    "        model.train()\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            optimizer.zero_grad()\n",
    "            loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            train_loss_set.append(loss.item())    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "        print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8VPcqYP0Y8eO"
   },
   "outputs": [],
   "source": [
    "def validate_BERT(val, model):\n",
    "    y_val = val.result.values\n",
    "    X_val = val.text.values\n",
    "\n",
    "    MAX_LEN = 128\n",
    "    X_val = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in X_val]\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    tokenized_val = [tokenizer.tokenize(sent) for sent in X_val]\n",
    "    val_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_val]\n",
    "    val_ids = pad_sequences(val_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    batch_size = 32\n",
    "    val_masks = []\n",
    "    for seq in val_ids:\n",
    "      seq_mask = [float(i>0) for i in seq]\n",
    "      val_masks.append(seq_mask)\n",
    "\n",
    "    validation_inputs = torch.tensor(val_ids).to(torch.int64)\n",
    "    validation_labels = torch.tensor(y_val).to(torch.int64)\n",
    "    validation_masks = torch.tensor(val_masks).to(torch.int64)\n",
    "\n",
    "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "    validation_sampler = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "    \n",
    "    model.eval()\n",
    "    predictions , true_labels = [], []\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        predictions.append(logits)\n",
    "        true_labels.append(label_ids)\n",
    "    \n",
    "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "    flat_true_labels = [item for sublist in true_labels for item in sublist]\n",
    "    \n",
    "    score = f1_score(flat_predictions, val[\"result\"])\n",
    "    cv_acc[\"BERT\"] = score\n",
    "    print(\"CV Accuracy: \" + str(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iRK_DFFiY8eS"
   },
   "outputs": [],
   "source": [
    "def test_BERT(test, model):\n",
    "    X_test = test.text.values\n",
    "    X_test = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in X_test]\n",
    "\n",
    "    MAX_LEN = 128\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "    tokenized_test = [tokenizer.tokenize(sent) for sent in X_test]\n",
    "    test_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_test]\n",
    "    test_ids = pad_sequences(test_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "    batch_size = 32\n",
    "    test_masks = []\n",
    "    for seq in test_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        test_masks.append(seq_mask)\n",
    "\n",
    "    test_inputs = torch.tensor(test_ids).to(torch.int64)\n",
    "    test_masks = torch.tensor(test_masks).to(torch.int64)\n",
    "\n",
    "    test_data = TensorDataset(test_inputs, test_masks)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "    \n",
    "    model.eval()\n",
    "    predictions , true_labels = [], []\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "    \n",
    "    flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "    flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "    return flat_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fuQNwGxIY8eW"
   },
   "outputs": [],
   "source": [
    "model5 = train_BERT(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "-5L-QvsYpDFp",
    "outputId": "ec882cbe-68d4-4287-dc86-462a9d7bd752"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV Accuracy: 0.9793138565336388\n"
     ]
    }
   ],
   "source": [
    "validate_BERT(test, model5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2Ny7fe0Y8ec"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>CV Scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BERT</td>\n",
       "      <td>0.979314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Linear SVM</td>\n",
       "      <td>0.967104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>FastText</td>\n",
       "      <td>0.959241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.957526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bi-LSTM</td>\n",
       "      <td>0.947977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  CV Scores\n",
       "0                 BERT   0.979314\n",
       "1           Linear SVM   0.967104\n",
       "2             FastText   0.959241\n",
       "3  Logistic Regression   0.957526\n",
       "4              Bi-LSTM   0.947977"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_acc = {k: v for k, v in sorted(cv_acc.items(), key=lambda item: item[1], reverse=True)}\n",
    "data = pd.DataFrame({'Model':list(cv_acc.keys()), 'CV Scores':list(cv_acc.values())})\n",
    "data"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled4.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
